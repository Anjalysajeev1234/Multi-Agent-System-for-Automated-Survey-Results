# =============== General =================
seed: 42

# Pick ONE block below by uncommenting. Keep the other commented for safety.

# -------- DEV (use with dev_groundtruth.json for local eval) --------
data:
  documents_path: "./data/documents.jsonl"
  questions_path: "./data/dev.json"          # switch to test.json for final run
  embeddings_path: "./data/id_to_embedding.npz"   # optional; not used for queries yet

# -------- TEST (10 questions, final/private) --------
# data:
#   documents_path: "./data/documents.jsonl"
#   questions_path: "./data/test.json"
#   embeddings_path: "./data/id_to_embedding.npz"

# =============== Index build ===============
index:
  output_dir: "./artifacts/index"
  force_rebuild: false

  # Sparse TF-IDF (always built; used for cosine & dedup)
  build_sparse: true

  # Whoosh BM25 (recommended). Build once, then reused.
  build_whoosh: true
  whoosh_index_dir: "./artifacts/whoosh_index"

  # Dense FAISS (optional; not used for queries in this config)
  build_dense: false
  faiss_type: "hnsw"          # "flat" | "hnsw" (ignored if build_dense=false)

# =============== TF-IDF settings (for index + scoring) ===============
tfidf:
  ngram_min: 1
  ngram_max: 1
  max_features: 80000
  min_df: 2

# =============== Whoosh settings (BM25) ===============
whoosh:
  index_dir: "./artifacts/whoosh_index"
  hits_per_query: 800           # per fan-out query

# =============== Retrieval ===========================
retrieval:
  # Use BM25 if available; fallback to TF-IDF if Whoosh missing
  sparse_backend: "bm25"        # "bm25" | "tfidf" | "both"
  top_k_sparse: 1000            # size for fused shortlist before dedup
  min_candidates: 300           # if fewer retrieved, we do a bounded TF-IDF fallback

  # Fusion across [question] + [question + each option]
  fusion: "rrf"
  rrf_k: 60

# =============== Filtering / Dedup ====================
filtering:
  dedup_sim_thresh: 0.985       # TF-IDF cosine threshold for near-duplicate removal

# =============== Labeling (stance) ====================
labeling:
  provider: "llm"               # "llm" | "cosine"
  model: "Qwen/Qwen2.5-7B-Instruct-Turbo"   # Together serverless, â‰¤$0.30/1M
  batch_size: 8
  max_output_tokens: 512
  temperature: 0.2              # helps stable JSON parsing
  top_docs_for_label: 150       # only label the top-precision subset

# =============== Calibration ==========================
calibration:
  dirichlet_alpha: 0.2          # smoothing on option counts
  temperature: 0.85             # softens over-confidence

# =============== Supports =============================
supports:
  size: 100                     # MUST be exactly 100 unique IDs per question

# =============== Run / Outputs ========================
run:
  output_csv: "./artifacts/submission_js_dev.csv"    # change to *_test.csv for test runs
  audit_log: "./artifacts/audit_dev.jsonl"           # change to *_test.jsonl for test runs
  per_question_time_cap_sec: 0.0                     # 0 disables; set e.g. 20.0 if you want a guardrail
